\chapter{Implementation}

This chapter will go through all of the implementation details of each demodulator implemented. They will be presented in order of complexity, with the more intricate ones presented last. This also will introduce them mostly chronologically since naive approaches allowed for more insight to be gained into the javAX25 software package before implementing more complicated algorithms. In addition to giving a brief overview of each implementation some performance data will be provided, but all of the data will be presented in the Results Chapter. As mentioned in the Demodulation Techniques Chapter, javAX25 was selected because of its availability and as such all of these implementation can be found in the author's Fork of the javAX25 GitHub repository in the following reference \cite{myJavAX25}. 

\section{Strict Zero Crossing Demodulator}
This approach used the technique of finding zero crossings and then using those to determine the period; from the period, the frequency was then calculated. For 1200Hz and 2200Hz tones zero crossings are expected every 833us and 455us respectively. If the resulting frequency after calculations was above 1700Hz it was assumed that a mark was present in the signal and if lower than 1700Hz a space must be present. Each zero crossing was found by determining if the signal was negative and changed to positive or if it was positive and changed to negative. Although this algorithm was only able to decode a little over half of the packets as some of the other algorithms, it proved to be an important stepping stone into javAX25 and allowed for preparation into restructuring the project for added modularity of the filtering. With the first implementation, Toledo's filters were not used and instead the previous three samples were averaged as a method of filtering to remove sample to sample noise.

\section{Floating Ground Zero Crossing Demodulator}
Building on the strict zero crossing algorithm the next zero crossing demodulator tried to use some more intelligence in finding the zero crossings through additional processing. One reason that the strict zero crossing approach was thought to have relatively poor results was due to the previously introduced challenge of DC offset. If the signal doesn't actually cross zero then it will be very hard to find the zero crossings. This new zero crossing method keeps a window of history (it was arbitrarily chosen to be one bit period) and from this collection of samples the average is taken to use this as the ground - or zero value. Instead of checking to see if the signal crosses zero, the signal is analyzed for going from either above to below or below to above this average value. This ended up having worse results than the strict zero crossing demodulator. This was due in part to the fact that 2200Hz signals - even when properly centered around zero - will not have an average of zero since it does not complete two fill periods within on bit period, which tainted the average.

\section{Windowed Zero Crossing Counting Demodulator}
With a good handle on utilizing zero crossing, a new approach was taken to keeping history. Instead of using the history to calculate where "zero" is, a new question was asked, "What if how many zero crossings within one period is observed?" If a window slightly shorter than one bit period is selected, then if there are only two crossings within the window that will correspond to a 1200Hz symbol being present. More crossings than two means that a 2200Hz symbol must be present. The thought behind taking this approach is that it would give some additional resiliency to noise by finding the average during that bit period through utilizing multiple zero crossing instead of individually analyzing every zero crossing. 

\section{Peak Detection Demodulator}
After making a simple zero crossing overly complicated, it was decided that maybe a different approach should be taken, specifically to look at a different part of the signal. It was considered that perhaps better performance could be achieved by looking at the peaks in the signal instead of the noisy zero crossing around ground, or not around ground if there are DC offset problems. Conveniently, the difference between two consecutive peaks will be equal to the period of the underlying signal. Although the methodology is the same as the zero crossing for converting the period to the actual frequency, it was perceived that this would give better results. It turns out that this method did not work as well as hoped due to the fact that local peaks were commonly discovered from the noise instead of the actual peak in the transmitted signal. More analysis into selecting a proper filter may give this approach better results.

\section{Derivative Zero Crossing Demodulator}
After a failure with the peak detection demodulator, a new approach was taken to finding "peaks." Instead of actually looking for the peaks, the zero crossing demodulator was revisited with a new spin. Instead of using the raw samples for determining the frequency using zero crossings, the derivative was to be used. The derivative was calculated by doing the same averaging as in the strict zero crossing approach and then subtracting the current average from the average two samples prior. It was thought that this would solve the DC offset problem for sure, but it turns out that this was not the larger problem. The problem was with using the zero crossing approach and this derivative implementation ended up having very similar results to the strict zero crossing with which one was better changing based off of pre-filtering.

\section{Goertzel Filter Demodulator}
Finally moving away from approaches utilizing zero crossing methodologies, an approach using a Goertzel filters was implemented. The implementation was very simple and corresponds with that outlined in the Demodulation Techniques Chapter. Since it has to be applied onto a set of data, a window size that was equal to one bit period was originally selected so as to make sure that the data being processed was only that of one frequency, but after analyzing the effect of the window size on performance, a window size of slightly longer than a bit period ended up being better. The optimal size was tested to be 135 percent of a bit period, and the reason why this worked better is because it gave more signal in the window for the filter to lock to. Essentially, the window was only extended 18 percent on each side of a bit period. This over-extension of the window is what led to being able to exceed the performance of the original correlator on unfiltered data.

\section{Phase Locked Loop Demodulator}
Next, the PLL demodulator was implemented. Using Lutus's python based software PLL initial testing was performed to see how it would work for tracking AX.25 signals \cite{Lutus2011}. Once the parameters were tuned sufficiently that it seemed to be staying locked onto the signal, it was ported over to java and actually run as a demodulator. Once inside of the javAX25 framework additional tuning was done programmatically instead of manually to further fine tune the performance. The final results were that it was not the winner, but comparable to the other top contenders, the Goertzel filter and correlation.

\section{Mixed Preclocking Demodulator}
Finally with numerous simple algorithms implemented it was time to try something much more complicated and also only possible in software. This approach and the name preclocking comes from an abbreviation for predetermined clocking where packets are analyzed a whole packet at a time. The start and end are found and then the clocking, and hence bit boundaries, are predetermined before the actual demodulation takes place on a baud by baud basis as opposed to a sample by sample basis. Each one of the preceding algorithms was on a sample by sample basis, meaning they had to make their best determination of bits elapsed using a little bit of history.

There are five steps to the demodulation in the Mixed Preclocking Demodulator. It was speculated that processing one packet at a time with the correct clocking to demodulate bit by bit would allow for very accurate demodulation.
\begin{enumerate}
\item Flags are found in the signal so that the demodulation can happen one packet at a time instead of just blindly trudging forward through the packet sample by sample.
\item The derivative of the whole packet is taken to  determine the zero crossings.
\item Frequency transitions are extrapolated from the derivative data.
\item The frequency transitions found in the packet are used to determine the clocking or bit boundaries.
\item The tone demodulation is done on a baud by baud basis.
\end{enumerate}

Although it was hoped that the results would be phenomenal, there were so many different methodologies being used that this preclocking demodulator was very difficult to tune. For instance the flags were found using the correlation approach, the transitions using a derivative, and the final demodulation using the zero crossings. What were thought to be the advantages ended up being the challenges, but as predicted it did well enough to still be considered one of the successful implementations. The intricate nature of this demodulator made it delicate, which was noticed during the testing through the fact that it would not decode any packets unless a bandpass filter was used on the incoming data.

\section{Goertzel Preclocking Demodulator}
After the first attempt at a preclocking approach, it was thought that perhaps using only one methodology to perform all the different steps of demodulation would be at the very least simpler, and ideally better. The perception that it might be better came from the fact that there was only one item to tune, the Goertzel Filter which had already shown good performance. Instead of having to worry about noise affecting zero crossings and the derivative potentially adding emphasis problems, only the filter had to be considered. Unfortunately, the number of packets that this method decoded was not as many as the first Goertzel approach or the previous preclocking. This was due to the fact that even though there was one underlying algorithm it was used in three separate instances, and each needed slightly different tuning. The three instances were for flag detection, frequency transition detection, and the the final bit by bit demodulation.

\section{Goertzel Exhaustive Preclocking Demodulator}
The final algorithm implemented was just a manner of verification, and another one that could be performed only in software. Instead of analyzing packets one at a time using flags as the start and end points, a whole array of data that had a length equal to the number of samples that a packet of the maximum length would have. Every time a few more samples came in, every single clocking was attempted on the large array of data just to see what packets could be decoded by exhaustively searching through the data. In terms of run time, this algorithm took much longer. For instance, the mixed preclocking and original Goertzel preclocking took 3 minutes and 5 seconds and 2:36 respectively to run, while this exhaustive search took 26:48 on the 25:49 Track 1 of the test suite. This means that a 2.1Ghz Intel i7 (i7-3612QM) could not process the audio file in less time than elapses during the content of the audio file. With that being the case, the result is that with live data this approach would not work since it would continuously fall behind. Gratefully, this approach only decoded an additional 15 packets that the Correlation, original Goertzel (non-preclocking), and PLL did not decode. This result could be used to make the argument that the few more packets decoded is not worth the vast number more CPU cycles it take to achieve it.

