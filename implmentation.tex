\chapter{Implementation}

This chapter will go through all of the implementation details of each algorithm implemented. They will be presented in order of simplicity, with the more intricate ones presented last. Before diving into each of implementations, some more information will be presented on the javaAX25 software suite created by Sivan Toledo, which is the basis that this project has it is foundations in order to be able to analyze these different approaches.

\section{javaAx25}

As mentioned earlier, one of the software demodulation approaches is Sivan Toledo's JavaAX25. The advantage of using Toledo's AX.25 package for benchmarking different algorithms is that due to the code structure and package hierarchy it makes it simple to interchange different demodulation algorithms. Also, the software is hosted on github making it convenient to access the repository. The next few paragraphs will give an overview of the features that Toledo’s software package has available in it with more detail on why this was a good code base to use as a foundation for this project \cite{Toledo2012,javax25github}.

JavaAX25 is a comprehensive package for doing software based modulation and demodulation of APRS packets - of course it does have AX25 in its name, so it is good that it lives up to its name. It includes packages for interfacing with radios, sound cards, and other standard packet programs that are used on computers. One example for interfacing with other programs is that there is a plugin to use JavaAX25 with AGW Packet Engine. In addition to having all of the items that are needed to be able to do the AX.25 modulation, demodulation, and interfacing with hardware, there is also a test application. Although all of these features were included, the three primarily used in this project were the modulation, demodulation, and testing.

The modulation portion coupled with the testing application for generating wav audio files. This was used to be able to generate the test file that had 200 packets in it for benchmarking the different demodulators.

The demodulation portion was the place where the most work was done. Through implementing an interface multiple demodulators could be interchanged and there performances compared. The first algorithm, was the one that Toledo implemented the JavaAX25 package with which was based off or correlation. To get more information about how a correlation algorithm works please refer to the corresponding previous section. From this correlation algorithm others, described individually bellow, were inserted into the demodulation path and tested.


%SIVAN CORRELATION?
%The algorithm that Toledo is currently using for the demodulation is correlation based. This is done by correlating the input signal with both a 1200Hz and a 2200Hz sine wave and seeing which of the two the input signal correlates with more. Once the correlation with each is established he filters the correlation data in order to smooth the results and make it easier to be able to pull the correct frequency out of the calculations. However, before doing any of the correlation calculations the data is passed through a band pass filter centered about 1700Hz which can be seen in \ref{SivanBandPassFilter}.

%%%FIGURE OF BANDPASS FILTER FROM SIVAN.

%Software demodulation provides a low cost alternative to hardware demodulators since users can run the software on hardware that they already possess. For example APRSDroid (an application in the Google’s Android Play Store) directly imports JavaAX25’s software demodulation \cite{APRSdroid}. 

%Talk about filtering and include filter (((FIGURE)))


The implementation started with a naive approach that allowed for more insight to be gained into the JavaAX25 software package. Some of the results and features that were found have been presented in the previous section. The first algorithm implemented was a Modified Zero Crossing Algorithm explained in section 4.1. Section 4.2 discusses our strict zero crossing. Section 4.3 discusses the windowed zero crossing. Section 4.4 discusses the peak detection algorithm. Our final implementation is a Preclocking demodulation algorithm and is discussed in section 4.5.

\section{Modified Zero Crossing}

As the first implementation there were some assumptions that were made that worked as a disadvantage to the demodulation. The thought behind this algorithm was to set a threshold around zero and when the value rises above this threshold centered on zero or falls below, determine that a zero crossing occurred. The time between subsequent zero crossings can be calculated in terms of the number of samples and the sample rate of the incoming audio can be used to figure out the time between the zero crossings in seconds.

One can calculate the time between zero crossings by realizing that they happen every p radians or twice in one frequency period. The period T can be calculated using T = 1 / f where f is the frequency. Using this information and the knowledge that the two tones are 1200Hz and 2200Hz one should expect zero crossings every 833µs and 455µs respectively. This logic of extrapolating the frequency from the zero crossing spacing of the signal is used for this as well as the next two algorithms presented in sections 4.2 and 4.3.

Upon implementing this algorithm there was an apparent disadvantage to using the threshold around zero. The original thought with this threshold is that it would help to get rid of erroneous zero crossings caused when there was low level noise on the signal. However, this buffer was not at all helpful since it doubled our opportunity for error on a zero crossing. If there was noise on the signal both right before entering the threshold and right before exiting this threshold it allowed for two separate contributions to error on figuring out exactly what sample number the crossing was at. Noticing this flaw the next algorithm, called the strict zero crossing, is exactly as the name implies; a strict zero crossing demodulation algorithm instead of having a threshold around zero.

\section{Strict Zero Crossing}

The strict zero crossing algorithm had advantages over the initial zero crossing algorithm implemented since it only has one area where error can occur. Unlike the algorithm in section 4.1 the zero crossing corresponds to exactly one value sample as opposed to relying on two samples in order to the back calculate the zero crossing.

The initial algorithm kept track of whether the signal was high (above zero) or low (below zero) and then when it transitioned to the opposite this was considered a zero crossing. The spacing between crossings was then used to calculate what frequency must be present in the signal using the logic explained in section 4.1. The initial results for this algorithm were good with clean signals, but once any noise was introduced, it still suffered from the same problems as the first Zero Crossing algorithm which was that the noise was bumping the zero crossing values around.

One example of the zero crossing weakness to noise was having 1200Hz tones that looked like 2200Hz tones. This case arose when the noise made one of the 1200Hz crossings happen a little late and then the subsequent zero crossing for the 1200Hz crossing a little early. This now smaller time between what should be a 1200Hz crossing distance now instead looks like a 2200Hz tone. In order to alleviate some of the trouble caused by this random white noise some filtering was added. See Figure 3 for an example of noise corruption.

At first the filtering chosen was just an average of the two adjacent points. This had better results for some cases, but not for all. For instance, if the noise was just affecting one zero crossing used for the calculation of the frequency it helped, but if the noise was affecting both zero crossings that were needed to figure out the frequency then we found that moving up to an average of three points proved to have the best results in terms of total number of packets decoded. This does makes sense though since if the noise causes the signal to jump either above or below the true value and averaging the three adjacent points allowed for two of them to be very noisy and cancel each other out while still having one to keep the filtered value from the averaging as close as possible to the original signal.

In addition to doing a straight averaging of the three adjacent points, some effort was put into doing a weighted average. The results from the weighted average were not as good since the whole point was to try and filter out some of the noise. If specific points were weighted instead of doing the unweighted average it meant that the goal of the filtering through averaging was useless. This is because if more weight is put on any one of the value it means that the noise in that point could overwhelm the calculation, and as mentioned then throw off the point of the filtering which is to get a closer approximation to the original signal value.

\section{Windowed Zero Crossing}

Although the strict zero crossing was much better than the original zero crossing algorithm there were still improvements that needed to be made. The filtering that was done through averaging adjacent points helped, but in some extreme cases it was still not enough. The next algorithm relied on averaging the averages. Basically instead of just looking on a crossing to crossing basis, a sliding window about the length of a symbol period is used, and the crossings within that window counted. The thought was that this would place less dependence on each zero crossing being calculated correctly since it is only one data point and hopefully the other one or two in the window will make correctly determining the frequency of the tone present in the signal more accurate. 

In order to make it more deterministic how many crossings were expected in the window a window size slightly less than one baud was chosen. The reason for this is that for a 1200Hz tone if the size of the window is just one sample greater than a symbol period there may be 3 crossings in the window since the signal is 1200 bps and hence the 1200Hz tone can complete one full period during the time elapsed in one symbol period. Making the window slightly less, 90% the size, than the symbol period ensures that noise will not interfere with the crossing count and that within the window there will only ever be 2 crossings if a 1200Hz tone is present in the window. If there are less than two crossings in the window then there is just noise / a DC offset and greater than 2 crossings is assumed to be a 2200Hz tone. This may seem like a far leap, what if there is noise at 20,000Hz? This was handled by requiring a minimum time to elapse between crossings that was a little less than that expected for subsequent 2200Hz crossings. This does mean that the algorithm will favor 2200Hz tones over 1200Hz tones when in the noise, but it still picks up the 1200Hz tones fairly well.

This theory and approach is all fine and dandy up until actually applied. Although, it originally seemed like a good idea, there were problems with setting all the values “correctly.” For instance there are three apparent variables that can be tweaked in order to change the performance: 1. the size of the window, 2. how many crossings should be expected within the selected window size, and 3. the amount of time to wait before allowing a zero crossing the be considered a zero crossing. Modifying these three parameters showed many different results in terms of the total number of packets decoded by the algorithm. For instance in the Open Tracker 3 test file that had 40 packets in it (more information on it in section 5) the algorithm could decode 39 of the packets with the window size set to 90% of a symbol length and the number of successfully decoded packets would drop if the window size was decreased below 87% or increased above 93%.

\section{Peak Detection}

The peak detection demodulator was developed to try and mitigate some of the problems observed with the strict zero crossing method. This takes advantage that peaks will occur on exact sample intervals while zero crossings samples don’t typically don’t occur directly at the crossing. This delay for the zero crossing can lead to ambiguity by a sample or two on either side of the zero crossing. The number of samples between the zero crossings can be either expanded or compressed by a few samples causing misidentification. By using peak detection this ambiguity could be further reduced. 

The algorithm uses the absolute value of the time domain to move the troughs to be peaks so transitions can be detect if they occurred during the peak to trough transition using the peak algorithm.  A three sample average is used for each incoming sample to help smooth the noise present. Three samples blends the noise while still preserving the integrity of the shape of the sinusoidal signal. The current sample is then compared to the previous local peak and stored if it’s higher along with the sample index number. If the two previous samples are consecutively decreasing a peak has occurred and the signal is moving in the downward direction. The handlePeakDetection() method is then called and after processing is completed the resets the local peak, local peak index, samples since the previous peak, and moved into the decreasing state. While in the decreasing state, the algorithm determines if the state has changed to increasing when three consecutive samples are larger than the previous. Once the increasing state has been entered, the algorithm repeats waiting for a peak.

The handlePeakDetection method first determines the number of samples between the current and previous peaks. This value is then subtracted from the previous number of samples between peaks. If the difference is greater than a threshold a transition has occurred. Once a transition has been detected, the number of bits is determined and the previously mentioned packet creating process in the JavaAX25 documentation is utilized.

The peak detection demodulation method was able to detect extremely clean source signals, but struggled against real world signals. It was difficult to tweak the algorithm to decode packets. Changing the difference threshold would help in some cases, while hurt in others making no perfect solution. Many packets had only a few bit errors. This approach could maybe be improved by averaging the previous peak periods to more clearly determine a transition occurrence, but due to time constraints more effort was put into the next demodulation technique. 

\section{Preclocking Demodulation}

Using the information collected thus far and the now greater knowledge of the software package as a whole the next algorithm is considerably more complicated. There are size different steps to the algorithm. First, filter the data to remove noise and higher frequency components and make the signal smoother. Second, look for flags in the signal so that the demodulation can happen one packet at a time instead of just blindly trudging forward through the packet sample by sample. Third, take the derivative of the whole packet to and determine the zero crossings. Fourth, frequency transitions are extrapolated from the derivative data. Fifth, the frequency transitions of the packet clocking are calculated and finally, sixth the tone demodulation is done on a baud by baud basis.

The original goal with this much more complicated algorithm than its predecessors was to take advantage of the clocking of the original signal. Using the clocking in the digitally encoded signal, more confidence can be instilled into making sure that every symbol gets demodulated correctly. The filtering and flag finding is done using the preexisting code that Toledo wrote, so not that many details will be included about it here. Instead the explanation of the inner working will start with why the derivative of the input signal was used. The nice thing about using the derivative in this algorithm instead of the original signal is that it solves two previously encountered problems: 1. the DC offset problem and 2. the emphasis problem. DC offset is when the oscillation of the frequency doesn’t occur around zero, but has been biased around a different DC voltage. This can occur in hardware due to the different strengths of the two signals. If a 1200 Hz tone has a higher magnitude than the 2200 Hz tone the 2200 Hz can be off centered on the voltage at the transition time. This can be observed in Figure 4. The Bell 202 signal is typically FM modulated onto a carrier for RF transmission and FM Modulation tends to attenuate higher baseband frequencies. In audio systems the higher frequencies are amplified before FM modulation and then attenuated after demodulation on the receiver. This creates a stronger 2200 Hz tone. There is no standard practice among amateur APRS users as to how/when to emphasis or de-emphasis packets. This creates the need diverse detection criteria to address the biasing of either 2200 Hz or 1200 Hz without knowing ahead of time which emphasis state has occurred. See Figures 5 and 6 for examples. Due to the nature of the derivative the DC offset problem is solved very literally, but solving emphasis problems is not necessarily as obvious, however it still shows improvements over the original signal. See Figure 7.

Once the data is filtered through both the direct FIR band pass filtering and through the derivative the frequencies seen in the packet are stored and using linear interpolation the exact point that each frequency transition occurred at is stored. Since frequency transitions will only happen along baud borders this allows for the clocking to be extrapolated from these transitions. The clocking is then determined in terms of offset in samples from the start of the possible packet. This is done through going through each one of the possible clockings (for a 48000 sample per second audio source this works out to 40 possible alignments) and selecting the sample offset number that minimizes the square distance between that perceived clocking and the observed transitions. 

Once the clocking is determined the frequency data that was calculated using the zero crossing from the derivative is used in order to figure out the actual tone that is contained within each baud. Two different approaches were tried to extrapolate the frequency out of the symbol window. First it was thought that taking the average of each one of the frequencies that fall within the baud period that just calculated would be best, but it became apparent that the filtering and derivative was not enough. Within decoding of legitimate packets frequencies greater than 5,000Hz were detected skewing the 1200Hz tones to look like 2200Hz when averaged. Another approach of just taking the one value directly in the middle of a baud period was used, but this didn’t prove to be worse. The minimum amount of time to pass between zero crossings in the algorithm was modified to try and get more accurate frequency results. Then the detected symbols were histogrammed to see how many determined frequencies were ambiguous between 1200-2200Hz. The result showed a clear division between the two frequencies and the threshold was set in the middle at 1700 Hz for the average of the frequencies inside the baud period as seen in Figure 8. The end results explained in more detail below show prove that this algorithm is on par with the original correlation algorithm and can decode some packets that it could not.
