\chapter{Implementation}

This chapter will go through all of the implementation details of each algorithm implemented. They will be presented in order of simplicity, with the more intricate ones presented last. This also will introduce them mostly chronologically since a naive approaches allowed for more insight to be gained into the JavaAX25 software package before implementing more complicated algorithms. In addition to giving a brief overview of each implementation some performance data will be provided, but all of the data will be presented in the Results Chapter. To see detailed 

\section{Strict Zero Crossing Demodulator}
This approach used the technique of finding zero crossings and then using those to determine the period. From the period the frequency was then calculated. For 1200Hz and 2200Hz tones zero crossings are expected every 833µs and 455µs respectively. If it was above 1700Hz it was assumed that a mark was present in the signal and if lower than 1700Hz a space must be present. The zero crossing were found by determined if the signal was negative and changed to positive or if it was positive and changed to negative. Although this algorithm was only able to decode a little over half of the packets as some of the other algorithms, it proved to be an important stepping stone into javAX25 and allowed for preparation into restructuring the project for added modularity of the filtering. With the first implementation Toledo's bandpass filters were not used and instead the previous three samples were averaged as a method of filtering to remove sample to sample noise.

\section{Zero Crossing Demodulator}
Building on the strict zero crossing this zero crossing demodulator tried to use some more intelligence in finding the zero crossings through additional processing. One reason that the strict zero crossing approach was thought to have relatively poor results was due to the previously introduced challenge of DC offset. If the signal doesn't actually cross zero then it will be very hard to find the zero crossings. This method keeps a window of history, it was arbitrarily chosen to be one bit period, and from this collection of samples the average is taken to use this as the baseline - or zero value. Instead of checking to see if the signal crosses zero, the signal is analyzed for going from either above to below or below to above this average value. This ended up having worse results than the strict zero crossing demodulator. This was due in part to the fact that 2200Hz signals even when properly centered around zero will not have an average of zero since it does not complete two fill periods within on bit period, tainting the average.

\section{Windowed Zero Crossing Demodulator}
With now having a good handle on utilizing zero crossing a new approach was taken to keeping history. Instead of using the history to calculate where "zero" is, what if how many zero crossings are within one period are observed. If a windows slightly shorter than one bit period is selected, then if there are only two crossings within that window it will correspond to a 1200Hz symbol being present. More crossings than two means that a 2200Hz symbols must be present. The thought behind taking this approach is that it would give some additional resiliency to noise by finding the average during that bit period through utilizing multiple zero crossing instead of individually analyzing every zero crossing. 

\section{Peak Detection Demodulator}
After making a simple zero crossing overly complicated, it was decided that maybe a different approach should be taken, specifically to look at a different part of the signal. It was considered that perhaps better performance could be achieved by looking at the peaks in the signal instead of the noisy zero crossing around ground, or not around ground if there are DC offset problems. The nice thing about this is that the difference between two consecutive peaks will be equal to the period of the underlying signal. Although the methodology is the same as the zero crossing for converting the period to the actual frequency it was perceived that this would give better results. It turns out that this method did not work as well as hoped due to the fact that local peaks were commonly discovered from the noise instead of the actual peak in the transmitted signal.

\section{Derivative Zero Crossing Demodulator}
After a failure with the peak detection demodulator a new approach was taken to finding "peaks." Instead of actually looking for the peaks, the zero crossing demodulator was revisited with a new spin. Instead of using the raw samples for determining the frequency using zero crossings, the derivative was to be used. The derivative was calculated by doing the same averaging as in the strict zero crossing approach and then subtracting the current average from the average two samples ago. It was thought that this would solve the DC offset problem for sure, but it turns out that this was not the larger problem. The problem was with using the zero crossing approach and this derivative implementation ended up just having very similar results to the strict zero crossing.

\section{Goertzel Filter Demodulator}
Finally moving away from approaches utilizing zero crossing methodologies, an approach using a Goertzel filters was implemented. The implementation was very simple and corresponds with that outlined in the Demodulation Techniques Chapter. Since it has to be applied onto a set of data, originally a window size was selected that was equal to one bit period so as to make sure that the data being processed was only that of one frequency, but after analyzing the effect of the window size on performance, a window size of slightly bigger than a bit period ended up being better. The optimal size was tested to be 135 percent of a bit period, and the reason why this worked better is because it gave more signal in the window for the filter to lock onto and essentially the window was only extended 18 percent on each side of a bit period. This over extension of the window is what led to being able to exceed the performance of the original correlator on unfiltered data.

\section{Phase Locked Loop Demodulator}
Next, the PLL demodulator was implemented. Using Lutus's python based software PLL initial testing was performed to see how it would work for tracking AX.25 signals \cite{Lutus2011}. Once the parameters were tuned sufficiently that it seemed to be staying locked onto the signal it was ported over to java and actually run as a demodulator. Once inside of the javAX25 framework additional tuning was done programatically instead of manually to further fine tune the performance. The final results were that it was not the winner, but comperable to the other top contenders, correlation and Goertzel filter.

\section{Mixed Preclocking Demodulator}
Finally with numerous simple algorithms implemented, or at least they may appear that way due to their relatively few lines of code, it was time to try something much more complicated. Something that would only be possible in software to see if it would shine. This approach and name preclocking comes from an abbreviation for predetermined clocking where packets are analyzed a whole packet at a time. The start and end are found and then the clocking and hence bit boundaries are predetermined before the actual demodulation takes place on a bit by bit basis as opposed to a sample by sample basis. Each one of the preceeding algorithms was on a sample by sample basis, meaning they had to make their best determination of bits elapsed using a little bit of history.

There are five different steps to the demodulation in the Mixed Preclocking Demodulator. First, flags are found in the signal so that the demodulation can happen one packet at a time instead of just blindly trudging forward through the packet sample by sample. Second, the derivative of the whole packet is taken to  determine the zero crossings. Third, frequency transitions are extrapolated from the derivative data. Fourth, the frequency transitions found in the packet are used to determine the clocking or bit boundaries. Finally, fifth the tone demodulation is done on a baud by baud basis. It was speculated that processing one packet at a time with the correct clocking to demodulate bit by bit would allow for very accurate demodulation.

Although, it was hoped that the results would be better, there were so many different methodologies being used that it was very difficult to tune. For instance the flags were found using the correlation approach, and the transitions using a derivative, and the final demodulation using the zero crossings. What were thought to be the advantages ended up being the challenges, but as predicted it did pretty well to still be considered one of the successful implementations. The intricate nature of this demodulator made it delicate which was noticed during the testing through the fact that it would not decode any packets unless a bandpass filter was used on the incoming data.

\section{Goertzel Preclocking Demodulator}
After the first attempt as a preclocking approach, it was thought that perhaps only using one methodology to perform all the different steps of demodulation would be at the very least simpler, and hopefully better. The perception that it might be better came from the fact that there was only one item to tune, the Goertzel Filter. Instead of having to worry about noise affecting zero crossings and the derivative potentially adding emphasis problems, only the filter has to be considered. Unfortunately the number of packets that this method decoded was not as many as the first Goertzel approach, or the previous preclocking. This was due to the fact that even though there was one underlying algorithm it was used in three separate instances, and each wanted slightly different tuning. The three instances were for flag detection, frequency transition detection, and the the final bit by bit demodulation.

\section{Goertzel Exhaustive Precklocking Demodulator} %TODO fill in numbers below...
The final algorithm implemented was just a manner of verification, and another one that could only be performed in software. Instead of analyzing packets one at a time using flags as the start and end points, a whole array of data that had a length equal to the number of samples that a packet if the maximum length would have. Every time a few more samples came in, every single clocking was attempted on the large array of data just to see what packets could be decoded by exhaustively searching for data. The performance of this algorithm in terms of time to run was much longer. For instance the mixed preclocking and original Goertzel preclocking took XXX and XXX to run respectively while this exhaustive search took XXX on the 25 minute 49 second Track 1 of the test suite. This means that although a 2.1Ghz Intel i7 (i7-3612QM) could keep up in real time it is close enough that any kind of embedded system at the time of this writing would not have the computational power to keep up with live data. Gratefully, this approach only decoded an additional 15 packets that the Correlation, original Goertzel (non-preclocking), and PLL did not decode. This result could be used to make the argument that the few more packets produced is not worth the vast number more CPU cylces it take to acheive it.

