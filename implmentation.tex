\chapter{Implementation}

This chapter will go through all of the implementation details of each algorithm implemented. They will be presented in order of simplicity, with the more intricate ones presented last. This also will introduce them mostly chronologically since a naive approaches allowed for more insight to be gained into the JavaAX25 software package before implementing more complicated algorithms. In addition to giving a brief overview of each implementation some performance data will be provided, but all of the data will be presented in the Results Chapter. To see detailed 

\section{Strict Zero Crossing Demodulator}
This approach used the technique of finding zero crossings and then using those to determine the period. From the period the frequency was then calculated. For 1200Hz and 2200Hz tones zero crossings are expected every 833µs and 455µs respectively. If it was above 1700Hz it was assumed that a mark was present in the signal and if lower than 1700Hz a space must be present. The zero crossing were found by determined if the signal was negative and changed to positive or if it was positive and changed to negative. Although this algorithm was only able to decode a little over half of the packets as some of the other algorithms, it proved to be an important stepping stone into javAX25 and allowed for preparation into restructuring the project for added modularity of the filtering. With the first implementation Toledo's bandpass filters were not used and instead the previous three samples were averaged as a method of filtering to remove sample to sample noise.

\section{Zero Crossing Demodulator}
Building on the strict zero crossing this zero crossing demodulator tried to use some more intelligence in finding the zero crossings through additional processing. One reason that the strict zero crossing approach was thought to have relatively poor results was due to the previously introduced challenge of DC offset. If the signal doesn't actually cross zero then it will be very hard to find the zero crossings. This method keeps a window of history, it was arbitrarily chosen to be one bit period, and from this collection of samples the average is taken to use this as the baseline - or zero value. Instead of checking to see if the signal crosses zero, the signal is analyzed for going from either above to below or below to above this average value. This ended up having worse results than the strict zero crossing demodulator. This was due in part to the fact that 2200Hz signals even when properly centered around zero will not have an average of zero since it does not complete two fill periods within on bit period, tainting the average.

\section{Windowed Zero Crossing Demodulator}
With now having a good handle on utilizing zero crossing a new approach was taken to keeping history. Instead of using the history to calculate where "zero" is, what if how many zero crossings are within one period are observed. If a windows slightly shorter than one bit period is selected, then if there are only two crossings within that window it will correspond to a 1200Hz symbol being present. More crossings than two means that a 2200Hz symbols must be present. The thought behind taking this approach is that it would give some additional resiliency to noise by finding the average during that bit period through utilizing multiple zero crossing instead of individually analyzing every zero crossing. 

\section{Peak Detection Demodulator}
After making a simple zero crossing overly complicated, it was decided that maybe a different approach should be taken, specifically to look at a different part of the signal. It was considered that perhaps better performance could be achieved by looking at the peaks in the signal instead of the noisy zero crossing around ground, or not around ground if there are DC offset problems. The nice thing about this is that the difference between two consecutive peaks will be equal to the period of the underlying signal. Although the methodology is the same as the zero crossing for converting the period to the actual frequency it was perceived that this would give better results. It turns out that this method did not work as well as hoped due to the fact that local peaks were commonly discovered from the noise instead of the actual peak in the transmitted signal.

\section{Derivative Zero Crossing Demodulator}
After a failure with the peak detection demodulator a new approach was taken to finding "peaks." Instead of actually looking for the peaks, the zero crossing demodulator was revisited with a new spin. Instead of using the raw samples for determining the frequency using zero crossings, the derivative was to be used. The derivative was calculated by doing the same averaging as in the strict zero crossing approach and then subtracting the current average from the average two samples ago. It was thought that this would solve the DC offset problem for sure, but it turns out that this was not the larger problem. The problem was with using the zero crossing approach and this derivative implementation ended up just having very similar results to the strict zero crossing.

\section{Goertzel Filter Demodulator}
Finally moving away from approaches utilizing zero crossing methodologies, an approach using a Goertzel filters was implemented. The implementation was very simple and corresponds with that outlined in the Demodulation Techniques Chapter. Since it has to be applied onto a set of data, originally a window size was selected that was equal to one bit period so as to make sure that the data being processed was only that of one frequency, but after analyzing the effect of the window size on performance, a window size of slightly bigger than a bit period ended up being better. The optimal size was tested to be 135 percent of a bit period, and the reason why this worked better is because it gave more signal in the window for the filter to lock onto and essentially the window was only extended 18 percent on each side of a bit period. This over extension of the window is what led to being able to exceed the performance of the original correlator on unfiltered data.

\section{Phase Locked Loop Demodulator}
Next, the PLL demodulator was implemented. Using Lutus's python based software PLL initial testing was performed to see how it would work for tracking AX.25 signals \cite{Lutus2011}. Once the parameters were tuned sufficiently that it seemed to be staying locked onto the signal it was ported over to java and actually run as a demodulator. Once inside of the javAX25 framework additional tuning was done programatically instead of manually to further fine tune the performance. The final results were that it was not the winner, but comperable to the other top contenders, correlation and Goertzel filter.

\section{Mixed Preclocking Demodulator}
Finally with numerous simple algorithms implemented, or at least they may appear that way due to their relatively few lines of code, it was time to try something much more complicated. Something that would only be possible in software to see if it would shine. 

There are five different steps to the demodulation in the Mixed Preclocking Demodulator. First, flags are found in the signal so that the demodulation can happen one packet at a time instead of just blindly trudging forward through the packet sample by sample. Second, the derivative of the whole packet is taken to  determine the zero crossings. Third, frequency transitions are extrapolated from the derivative data. Fourth, the frequency transitions found in the packet are used to determine the clocking or bit boundaries. Finally, fifth the tone demodulation is done on a baud by baud basis. It was speculated that processing one packet at a time with the correct clocking to demodulate bit by bit would allow for very accurate demodulation.

Although, it was hoped that the results would be better, there were so many different methodologies being used that it was very difficult to tune. For instance the flags were found using the correlation approach, and the transitions using a derivative, and the final demodulation using the zero crossings. What were thought to be the advantages ended up being the challenges, but as predicted it did pretty well to still be considered one of the successful implementations.  

\section{Goertzel Preclocking Demodulator}

\section{Goertzel Exhaustive Precklocking Demodulator}


\section{Preclocking Demodulation}


The original goal with this much more complicated algorithm than its predecessors was to take advantage of the clocking of the original signal. Using the clocking in the digitally encoded signal, more confidence can be instilled into making sure that every symbol gets demodulated correctly. The filtering and flag finding is done using the preexisting code that Toledo wrote, so not that many details will be included about it here. Instead the explanation of the inner working will start with why the derivative of the input signal was used. The nice thing about using the derivative in this algorithm instead of the original signal is that it solves two previously encountered problems: 1. the DC offset problem and 2. the emphasis problem. DC offset is when the oscillation of the frequency doesn’t occur around zero, but has been biased around a different DC voltage. This can occur in hardware due to the different strengths of the two signals. If a 1200 Hz tone has a higher magnitude than the 2200 Hz tone the 2200 Hz can be off centered on the voltage at the transition time. This can be observed in Figure 4. The Bell 202 signal is typically FM modulated onto a carrier for RF transmission and FM Modulation tends to attenuate higher baseband frequencies. In audio systems the higher frequencies are amplified before FM modulation and then attenuated after demodulation on the receiver. This creates a stronger 2200 Hz tone. There is no standard practice among amateur APRS users as to how/when to emphasis or de-emphasis packets. This creates the need diverse detection criteria to address the biasing of either 2200 Hz or 1200 Hz without knowing ahead of time which emphasis state has occurred. See Figures 5 and 6 for examples. Due to the nature of the derivative the DC offset problem is solved very literally, but solving emphasis problems is not necessarily as obvious, however it still shows improvements over the original signal. See Figure 7.

Once the data is filtered through both the direct FIR band pass filtering and through the derivative the frequencies seen in the packet are stored and using linear interpolation the exact point that each frequency transition occurred at is stored. Since frequency transitions will only happen along baud borders this allows for the clocking to be extrapolated from these transitions. The clocking is then determined in terms of offset in samples from the start of the possible packet. This is done through going through each one of the possible clockings (for a 48000 sample per second audio source this works out to 40 possible alignments) and selecting the sample offset number that minimizes the square distance between that perceived clocking and the observed transitions. 

Once the clocking is determined the frequency data that was calculated using the zero crossing from the derivative is used in order to figure out the actual tone that is contained within each baud. Two different approaches were tried to extrapolate the frequency out of the symbol window. First it was thought that taking the average of each one of the frequencies that fall within the baud period that just calculated would be best, but it became apparent that the filtering and derivative was not enough. Within decoding of legitimate packets frequencies greater than 5,000Hz were detected skewing the 1200Hz tones to look like 2200Hz when averaged. Another approach of just taking the one value directly in the middle of a baud period was used, but this didn’t prove to be worse. The minimum amount of time to pass between zero crossings in the algorithm was modified to try and get more accurate frequency results. Then the detected symbols were histogrammed to see how many determined frequencies were ambiguous between 1200-2200Hz. The result showed a clear division between the two frequencies and the threshold was set in the middle at 1700 Hz for the average of the frequencies inside the baud period as seen in Figure 8. The end results explained in more detail below show prove that this algorithm is on par with the original correlation algorithm and can decode some packets that it could not.
